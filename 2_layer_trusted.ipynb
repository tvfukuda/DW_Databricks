{"cells":[{"cell_type":"markdown","source":["# Create Parquet tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbb033a8-ccac-4d7e-adb9-2b8356bba6c2"}}},{"cell_type":"markdown","source":["## Configuration\n\nBefore executing with cell, add your name to the file:\n<a href=\"$./includes/configuration\" target=\"_blank\">\nincludes/configuration</a>\n\n```username = \"your_name\"```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d5b5349-c2e6-4725-b7c2-f75df2c0d135"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"034a3518-2207-48fa-b942-05e47d0a7160"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Reload data to dictionary of DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d8fb334-a0c2-4e5e-8e69-146b2ea8f749"}}},{"cell_type":"code","source":["# paht to CSV files\nfile_paths = [raw_path + northwind_file for northwind_file in northwind_files]\n\n# dictionary of DataFrames\nraw_data_dict = {}\n\n# reload data\nfor ix in range(len(file_paths)):\n  table_name = northwind_tables[ix]\n  file_path = file_paths[ix]\n  raw_data_dict[table_name] = spark.read.csv(file_path, header = True, inferSchema = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b79183be-1722-4ae2-aa5a-9f23b43541f8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create Parquet tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b165a4c-5367-4744-9b56-b6a0fa945e63"}}},{"cell_type":"markdown","source":["#### Step 1: Clean up\nFiles at `trusted_path` are removed.\n\nThen tables are removed.\n\nThis step assures the notebook is idempotent. It means the notebook can be executed multiple times and the result will be same - no error raised nor extra files saved.\n\nðŸš¨ **NOTE:** In this sample project, files are saved to Databricks File System (DBFS). The good practice is to save files to cloud storage. DBFS is used only for demo purpose."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f080da9c-9873-4c46-9630-8424a1a8b51b"}}},{"cell_type":"code","source":["dbutils.fs.rm(trusted_path, recurse=True)\n\nfor table_name in northwind_tables_trusted:\n  spark.sql(\n    f\"\"\"\n    DROP TABLE IF EXISTS {table_name}\n    \"\"\"\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f336977-448e-4e1e-a505-317db16e268f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Transform data\nTransformations were defined in dictionary `northwind_types`.\n<br><br>[Reference to transform with list comprehension](https://stackoverflow.com/questions/70005826/how-to-select-columns-and-cast-column-types-in-a-pyspark-dataframe)\n<br>[Reference to create columns `order_year`, `order_month` and `order_day`](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34176a6d-768f-41ea-b539-98099c966cdb"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ntrusted_data_dict = {}\nfor table_name in northwind_columns:\n  trusted_data_dict[table_name + '_trusted'] = raw_data_dict[table_name].select([col(c).cast(t).alias(a) for c, t, a, _ in northwind_columns[table_name]])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"032ed4d8-167f-4417-b859-55d8cd6cad06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# transform columns 'order_year', 'order_month' e 'order_day'\ntrusted_data_dict['orders_trusted'] = trusted_data_dict['orders_trusted'].withColumn('order_year', year(col('order_date')).cast('string'))\ntrusted_data_dict['orders_trusted'] = trusted_data_dict['orders_trusted'].withColumn('order_month', month(col('order_date')).cast('string'))\ntrusted_data_dict['orders_trusted'] = trusted_data_dict['orders_trusted'].withColumn('order_day', dayofmonth(col('order_date')).cast('string'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65bd572a-dd10-4b50-94a8-0d8775f481d1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Save files to `trusted` folder\n\n1. Use `.format(\"parquet\")`\n2. Table 'orders_trusted' is partitioned using columns ``order_year``, ``order_month``, ``order_day``"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e3a7765-d378-4077-90dd-77d4b71e56cc"}}},{"cell_type":"code","source":["for table_name in trusted_data_dict:\n  if table_name == 'orders_trusted':\n    # save partitioned table\n    table_name = 'orders_trusted'\n    (trusted_data_dict[table_name].write\n     .mode(\"overwrite\")\n     .format(\"parquet\")\n     .partitionBy(\"order_year\", \"order_month\", \"order_day\")\n     .save(trusted_path + table_name))\n  else:\n    # save unpartitioned table\n    (trusted_data_dict[table_name].write\n     .mode(\"overwrite\")\n     .format(\"parquet\")\n     .save(trusted_path + table_name))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e48c47de-5754-4e86-8dcd-b5750e8d58ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4: Register tables in metastore\nSpark SQL is used to register tables in metastore.\nTables are created in Parquet format and the data source is the `trusted` folder."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ab26514-86b6-48bb-8fe7-85006278c548"}}},{"cell_type":"code","source":["for table_name in trusted_data_dict:\n  spark.sql(\n    f\"\"\"\n    DROP TABLE IF EXISTS {table_name}\n    \"\"\"\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0f80748-3c77-44b6-a9d7-a1016bfe7a89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for table_name in trusted_data_dict:\n  spark.sql(\n    f\"\"\"\n    CREATE TABLE {table_name}\n    USING PARQUET\n    LOCATION \"{trusted_path}/{table_name}\"\n    \"\"\"\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1623b4d-aece-4d40-9f2e-d7558b1d35d9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 5: Verify if Parquet tables are in data lake\nCount table entries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce50638f-45ae-42c4-aff3-e1178f8fd3e5"}}},{"cell_type":"code","source":["for table_name in trusted_data_dict:\n  northwind_table = spark.read.table(table_name)\n  print(table_name + \":\", northwind_table.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea15adfe-1cb5-4bfc-b574-b03fa33e242b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"categories_trusted: 8\ncustomer_customer_demo_trusted: 0\ncustomer_demographics_trusted: 0\ncustomers_trusted: 91\nemployees_trusted: 9\nemployee_territories_trusted: 49\norder_details_trusted: 2155\norders_trusted: 0\nproducts_trusted: 77\nregion_trusted: 4\nshippers_trusted: 6\nsuppliers_trusted: 29\nterritories_trusted: 53\nus_states_trusted: 51\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["categories_trusted: 8\ncustomer_customer_demo_trusted: 0\ncustomer_demographics_trusted: 0\ncustomers_trusted: 91\nemployees_trusted: 9\nemployee_territories_trusted: 49\norder_details_trusted: 2155\norders_trusted: 0\nproducts_trusted: 77\nregion_trusted: 4\nshippers_trusted: 6\nsuppliers_trusted: 29\nterritories_trusted: 53\nus_states_trusted: 51\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Incorrect value in partitioned table (*orders_trusted*)**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33404718-4616-44e9-a9a9-e12b0296112f"}}},{"cell_type":"markdown","source":["#### Step 6: Partition registry\n\nFollowing good practices, a partitioned table was created. However, partitions created from existing data are not identified by Spark SQL and registrations in metastore are required.\n\n`MSCK REPAIR TABLE` is going to register partitions in Hive Metastore. [More information in this link.](https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-repair-table.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b5bff9b-9056-44e7-875a-f9d3412ad08a"}}},{"cell_type":"code","source":["spark.sql(\"MSCK REPAIR TABLE orders_trusted\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64ac80c0-d86c-4337-90ad-e782d2f7bcc7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 7: Count table entries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f07ed74e-d958-41d4-82db-4fa6c82b1629"}}},{"cell_type":"code","source":["for table_name in trusted_data_dict:\n  northwind_table = spark.read.table(table_name)\n  print(table_name + \":\", northwind_table.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c25f7afa-2064-4af0-9318-02faf18ca57f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"categories_trusted: 8\ncustomer_customer_demo_trusted: 0\ncustomer_demographics_trusted: 0\ncustomers_trusted: 91\nemployees_trusted: 9\nemployee_territories_trusted: 49\norder_details_trusted: 2155\norders_trusted: 830\nproducts_trusted: 77\nregion_trusted: 4\nshippers_trusted: 6\nsuppliers_trusted: 29\nterritories_trusted: 53\nus_states_trusted: 51\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["categories_trusted: 8\ncustomer_customer_demo_trusted: 0\ncustomer_demographics_trusted: 0\ncustomers_trusted: 91\nemployees_trusted: 9\nemployee_territories_trusted: 49\norder_details_trusted: 2155\norders_trusted: 830\nproducts_trusted: 77\nregion_trusted: 4\nshippers_trusted: 6\nsuppliers_trusted: 29\nterritories_trusted: 53\nus_states_trusted: 51\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["__Table *orders_trusted* with correct count__"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afae1338-ba84-49fb-9ce1-e3514f7cc1e6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2_layer_trusted","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3534961934021079}},"nbformat":4,"nbformat_minor":0}
